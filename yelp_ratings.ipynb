{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Yelp Rating Prediction Using Tensorflow</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Cleaning:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn.feature_extraction.text as sk_text\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess text function -> if you get nltk error:  open Anaconda prompt -> ipython -> import nltk -> nltk.download('popular') OR command line -> python -m nltk.downloader popular\n",
    "def preprocess_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = text.lower()                 # lowercase\n",
    "    text = re.sub(r'\\d+', '', text)     # remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text)    # remove extra whitespace\n",
    "    text = text.strip()                 # remove leading/trailing whitespace\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]          # remove stopwords\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens] # lemmatize\n",
    "    filtered_text = ' '.join(lemmatized_tokens)                                    # join back into text\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Get reviews*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                review_id                 user_id             business_id  \\\n",
      "0  KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA  XQfwVwDr-v0ZS3_CbbE5Xw   \n",
      "1  BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q  7ATYjTIgM3jUlt4UM3IypQ   \n",
      "2  saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A  YjUWPpI6HXG530lwP-fb2A   \n",
      "3  AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA   \n",
      "4  Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ  e4Vwtrqf-wpJfwesgvdgxQ   \n",
      "\n",
      "   stars  useful  funny  cool  \\\n",
      "0      3       0      0     0   \n",
      "1      5       1      0     1   \n",
      "2      3       0      0     0   \n",
      "3      5       1      0     1   \n",
      "4      4       1      0     1   \n",
      "\n",
      "                                                text                date  \n",
      "0  If you decide to eat here, just be aware it is... 2018-07-07 22:09:11  \n",
      "1  I've taken a lot of spin classes over the year... 2012-01-03 15:28:18  \n",
      "2  Family diner. Had the buffet. Eclectic assortm... 2014-02-05 20:30:30  \n",
      "3  Wow!  Yummy, different,  delicious.   Our favo... 2015-01-04 00:01:03  \n",
      "4  Cute interior and owner (?) gave us tour of up... 2017-01-14 20:54:15  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json('data/yelp_academic_dataset_review.json', lines=True, chunksize=1000) # smaller chunksize helps with memory issues\n",
    "df = pd.concat(df)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Clean reviews of businesses with 20 or more reviews*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   stars                                               text\n",
      "0      3  If you decide to eat here, just be aware it is...\n",
      "1      2  This is the second time we tried turning point...\n",
      "2      4  The place is cute and the staff was very frien...\n",
      "3      3  We came on a Saturday morning after waiting a ...\n",
      "4      2  Mediocre at best. The decor is very nice, and ...\n"
     ]
    }
   ],
   "source": [
    "# Convert all missing values and zeroes in specified column to median -> from labs with slight modifications\n",
    "def missing_median(df, name):\n",
    "    non_zero_values = df[name][df[name] != 0]\n",
    "    med = non_zero_values.median()\n",
    "    df[name] = df[name].replace(0, med)\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "business_review_count = df.groupby('business_id').size().reset_index(name='review_count')                        # count reviews for each business\n",
    "businesses_with_20 = business_review_count[business_review_count['review_count'] >= 20]                          # filter businesses with 20 or more reviews\n",
    "df = pd.merge(df, businesses_with_20, on='business_id', how='inner')                                             # merge with reviews to get only reviews for businesses with 20 or more reviews\n",
    "df_businesses = df.copy()                                                                                        # save a copy of the dataframe for 3-5 business analysis\n",
    "df_businesses2 = df.copy()                                                                                       # save a copy of the dataframe for 3-5 business analysis\n",
    "df = df.drop(['business_id', 'review_id', 'user_id', 'funny', 'cool', 'useful', 'date', 'review_count'], axis=1) # drop unnecessary columns\n",
    "missing_median(df, 'stars')                                                                                      # in case there are any reviews with 0 or missing stars, replace with median\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Get five random businesses with 20 or more reviews*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              business_id                       name  stars  review_count  \\\n",
      "0  Jv8lYSZPxY0rzkSpgo7BIw  Big Boyz Burgers and More    4.0            22   \n",
      "1  7k9qGQyytbGxpJTnwxK6Xg         QDOBA Mexican Eats    3.0            40   \n",
      "2  v_vqna00z6WqKcIJZDkbAw                    C Nails    4.5            61   \n",
      "3  zsQ1_PNV3KN0EWhAE-WV9g  Tohono Chul Garden Bistro    4.0           165   \n",
      "4  v2L2HnZzYvHPgFcVBg2TUw             Summerland Inn    2.0            56   \n",
      "\n",
      "                                          categories  \n",
      "0  American (Traditional), Diners, Breakfast & Br...  \n",
      "1  Restaurants, Event Planning & Services, Mexica...  \n",
      "2   Waxing, Hair Removal, Nail Salons, Beauty & Spas  \n",
      "3  American (New), Venues & Event Spaces, Restaur...  \n",
      "4  Hotels, Hotels & Travel, Bed & Breakfast, Even...  \n"
     ]
    }
   ],
   "source": [
    "path = './business_data/'\n",
    "filename_read = os.path.join(path, 'df_five_businesses_info.csv')\n",
    "if not os.path.exists(filename_read):\n",
    "    df_businesses = df_businesses.drop(['review_id', 'user_id', 'funny', 'cool', 'useful', 'date', 'review_count', 'text', 'stars'], axis=1) # drop unnecessary columns\n",
    "    unique_business_ids = df_businesses['business_id'].unique()                                                                              # get unique business ids\n",
    "    np.random.shuffle(unique_business_ids)                                                                                                   # shuffle the unique business ids\n",
    "    five_unique_business_ids = unique_business_ids[:5]                                                                                       # get first 5 unique business ids\n",
    "    five_unique_ids_df = pd.DataFrame(five_unique_business_ids, columns=['business_id'])                                                     # create a dataframe with the first 5 unique business ids\n",
    "    print('five unique business ids:', five_unique_business_ids)\n",
    "    business_info_df = pd.read_json('data/yelp_academic_dataset_business.json', lines=True, chunksize=1000)                                  # read business info\n",
    "    five_businesses_df = pd.concat([chunk[chunk['business_id'].isin(five_unique_business_ids)] for chunk in business_info_df])\n",
    "    five_businesses_df = five_businesses_df.drop(['address', 'city', 'state', 'postal_code', 'latitude', 'longitude', 'hours', 'attributes', 'is_open'], axis=1)\n",
    "else:\n",
    "    five_businesses_df = pd.read_csv(filename_read)\n",
    "print(five_businesses_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Save five businesses' dataframe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./business_data/\"\n",
    "filename_read = os.path.join(path, \"df_five_businesses_info.csv\")\n",
    "if not os.path.exists(filename_read):\n",
    "    filename_write = os.path.join(path, \"df_five_businesses_info.csv\")\n",
    "    five_businesses_df.to_csv(filename_write, index=False, encoding='utf-8') # using default encoding also worked -> used for next cell but wasn't helpful\n",
    "    print(\"Wrote file to {}\".format(filename_write))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Separate business reviews by business name*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              business_id  stars  \\\n",
      "0  Jv8lYSZPxY0rzkSpgo7BIw      5   \n",
      "1  Jv8lYSZPxY0rzkSpgo7BIw      4   \n",
      "2  Jv8lYSZPxY0rzkSpgo7BIw      5   \n",
      "3  Jv8lYSZPxY0rzkSpgo7BIw      5   \n",
      "4  Jv8lYSZPxY0rzkSpgo7BIw      5   \n",
      "\n",
      "                                                text  \\\n",
      "0  The double cheeseburger is delicious. The serv...   \n",
      "1  Tried this spot here while in town on business...   \n",
      "2  I had the pleasure of eating here today for th...   \n",
      "3  Read a review in the Riverfront Times so I dec...   \n",
      "4  I called in my ordered of Special Fried Rice o...   \n",
      "\n",
      "                        name  \n",
      "0  Big Boyz Burgers and More  \n",
      "1  Big Boyz Burgers and More  \n",
      "2  Big Boyz Burgers and More  \n",
      "3  Big Boyz Burgers and More  \n",
      "4  Big Boyz Burgers and More  \n"
     ]
    }
   ],
   "source": [
    "path = './business_data/'\n",
    "filename_read = os.path.join(path, 'df_business1_reviews.csv')\n",
    "if not os.path.exists(filename_read):\n",
    "    reviews_for_five_businesses = df_businesses2[df_businesses2['business_id'].isin(five_unique_business_ids)]\n",
    "    reviews_for_five_businesses = reviews_for_five_businesses.drop(['review_id', 'user_id', 'funny', 'cool', 'useful', 'date', 'review_count'], axis=1)\n",
    "    reviews_with_names_df = reviews_for_five_businesses.merge(five_businesses_df[['business_id', 'name']], on='business_id', how='left')\n",
    "    business_names = reviews_with_names_df['name'].unique()\n",
    "    business1_reviews_df = reviews_with_names_df[reviews_with_names_df['name'] == business_names[0]]\n",
    "    business2_reviews_df = reviews_with_names_df[reviews_with_names_df['name'] == business_names[1]]\n",
    "    business3_reviews_df = reviews_with_names_df[reviews_with_names_df['name'] == business_names[2]]\n",
    "    business4_reviews_df = reviews_with_names_df[reviews_with_names_df['name'] == business_names[3]]\n",
    "    business5_reviews_df = reviews_with_names_df[reviews_with_names_df['name'] == business_names[4]]\n",
    "else:\n",
    "    filename_read = os.path.join(path, 'df_business1_reviews.csv')\n",
    "    business1_reviews_df = pd.read_csv(filename_read)\n",
    "    filename_read = os.path.join(path, 'df_business2_reviews.csv')\n",
    "    business2_reviews_df = pd.read_csv(filename_read)\n",
    "    filename_read = os.path.join(path, 'df_business3_reviews.csv')\n",
    "    business3_reviews_df = pd.read_csv(filename_read)\n",
    "    filename_read = os.path.join(path, 'df_business4_reviews.csv')\n",
    "    business4_reviews_df = pd.read_csv(filename_read)\n",
    "    filename_read = os.path.join(path, 'df_business5_reviews.csv')\n",
    "    business5_reviews_df = pd.read_csv(filename_read)\n",
    "print(business1_reviews_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              business_id  stars  \\\n",
      "0  7k9qGQyytbGxpJTnwxK6Xg      5   \n",
      "1  7k9qGQyytbGxpJTnwxK6Xg      2   \n",
      "2  7k9qGQyytbGxpJTnwxK6Xg      5   \n",
      "3  7k9qGQyytbGxpJTnwxK6Xg      5   \n",
      "4  7k9qGQyytbGxpJTnwxK6Xg      2   \n",
      "\n",
      "                                                text                name  \n",
      "0  Just like Chipotle! Food was great, very filli...  QDOBA Mexican Eats  \n",
      "1  I love Qdoba but this is kinda dirty, especial...  QDOBA Mexican Eats  \n",
      "2  Best burrito place in the world. Better than a...  QDOBA Mexican Eats  \n",
      "3  I like it.  the enviornment is clean and food ...  QDOBA Mexican Eats  \n",
      "4  I eat here often and i love their tacos, but t...  QDOBA Mexican Eats  \n"
     ]
    }
   ],
   "source": [
    "print(business2_reviews_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              business_id  stars  \\\n",
      "0  v_vqna00z6WqKcIJZDkbAw      5   \n",
      "1  v_vqna00z6WqKcIJZDkbAw      1   \n",
      "2  v_vqna00z6WqKcIJZDkbAw      5   \n",
      "3  v_vqna00z6WqKcIJZDkbAw      5   \n",
      "4  v_vqna00z6WqKcIJZDkbAw      5   \n",
      "\n",
      "                                                text     name  \n",
      "0  I was looking for a new salon in the Westchase...  C Nails  \n",
      "1  Horrible service, had to go to another salon t...  C Nails  \n",
      "2  Great gel manicures! Current one is going on o...  C Nails  \n",
      "3  I have only gotten their shellac manicure so f...  C Nails  \n",
      "4  After reading reviews and looking around I fou...  C Nails  \n"
     ]
    }
   ],
   "source": [
    "print(business3_reviews_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              business_id  stars  \\\n",
      "0  zsQ1_PNV3KN0EWhAE-WV9g      3   \n",
      "1  zsQ1_PNV3KN0EWhAE-WV9g      4   \n",
      "2  zsQ1_PNV3KN0EWhAE-WV9g      5   \n",
      "3  zsQ1_PNV3KN0EWhAE-WV9g      3   \n",
      "4  zsQ1_PNV3KN0EWhAE-WV9g      5   \n",
      "\n",
      "                                                text  \\\n",
      "0  If you're looking for a restaurant to bring ou...   \n",
      "1  We decided to try again recently,and things ar...   \n",
      "2  We came across this brunch place & absolutely ...   \n",
      "3  Great atmosphere and food was flavorful. But t...   \n",
      "4  This is my favorite brunch place for sure! The...   \n",
      "\n",
      "                        name  \n",
      "0  Tohono Chul Garden Bistro  \n",
      "1  Tohono Chul Garden Bistro  \n",
      "2  Tohono Chul Garden Bistro  \n",
      "3  Tohono Chul Garden Bistro  \n",
      "4  Tohono Chul Garden Bistro  \n"
     ]
    }
   ],
   "source": [
    "print(business4_reviews_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              business_id  stars  \\\n",
      "0  v2L2HnZzYvHPgFcVBg2TUw      1   \n",
      "1  v2L2HnZzYvHPgFcVBg2TUw      1   \n",
      "2  v2L2HnZzYvHPgFcVBg2TUw      5   \n",
      "3  v2L2HnZzYvHPgFcVBg2TUw      3   \n",
      "4  v2L2HnZzYvHPgFcVBg2TUw      5   \n",
      "\n",
      "                                                text            name  \n",
      "0  I stayed one night here and I will never come ...  Summerland Inn  \n",
      "1  Things wrong with this room: large spider webs...  Summerland Inn  \n",
      "2  I like the place. It's got character. It was 1...  Summerland Inn  \n",
      "3  This place is interesting. It's very cute outs...  Summerland Inn  \n",
      "4  The Summerland Inn is so beautiful, and the ow...  Summerland Inn  \n"
     ]
    }
   ],
   "source": [
    "print(business5_reviews_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Save individual business review dataframes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './business_data/'\n",
    "filename_read = os.path.join(path, 'df_business1_reviews.csv')\n",
    "if not os.path.exists(filename_read):\n",
    "    filename_write = os.path.join(path, \"df_business1_reviews.csv\")\n",
    "    business1_reviews_df.to_csv(filename_write, index=False, encoding='utf-8')\n",
    "    print(\"Wrote file to {}\".format(filename_write))\n",
    "    filename_write = os.path.join(path, \"df_business2_reviews.csv\")\n",
    "    business2_reviews_df.to_csv(filename_write, index=False, encoding='utf-8')\n",
    "    print(\"Wrote file to {}\".format(filename_write))\n",
    "    filename_write = os.path.join(path, \"df_business3_reviews.csv\")\n",
    "    business3_reviews_df.to_csv(filename_write, index=False, encoding='utf-8')\n",
    "    print(\"Wrote file to {}\".format(filename_write))\n",
    "    filename_write = os.path.join(path, \"df_business4_reviews.csv\")\n",
    "    business4_reviews_df.to_csv(filename_write, index=False, encoding='utf-8')\n",
    "    print(\"Wrote file to {}\".format(filename_write))\n",
    "    filename_write = os.path.join(path, \"df_business5_reviews.csv\")\n",
    "    business5_reviews_df.to_csv(filename_write, index=False, encoding='utf-8')\n",
    "    print(\"Wrote file to {}\".format(filename_write))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Process text of business1 reviews and save for model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              business_id  stars                       name  \\\n",
      "0  Jv8lYSZPxY0rzkSpgo7BIw      5  Big Boyz Burgers and More   \n",
      "1  Jv8lYSZPxY0rzkSpgo7BIw      4  Big Boyz Burgers and More   \n",
      "2  Jv8lYSZPxY0rzkSpgo7BIw      5  Big Boyz Burgers and More   \n",
      "3  Jv8lYSZPxY0rzkSpgo7BIw      5  Big Boyz Burgers and More   \n",
      "4  Jv8lYSZPxY0rzkSpgo7BIw      5  Big Boyz Burgers and More   \n",
      "\n",
      "                                      processed_text  \n",
      "0  double cheeseburger delicious service welcomin...  \n",
      "1  tried spot town business great burger hand for...  \n",
      "2  pleasure eating today first time honestly one ...  \n",
      "3  read review riverfront time decided swing cuti...  \n",
      "4  called ordered special fried rice saturday tol...  \n"
     ]
    }
   ],
   "source": [
    "path = \"./business_data/\"\n",
    "filename_read = os.path.join(path, \"df_business1_reviews_processed.csv\")\n",
    "if not os.path.exists(filename_read):\n",
    "    business1_reviews_df['processed_text'] = business1_reviews_df['text'].apply(preprocess_text)\n",
    "    business1_reviews_df = business1_reviews_df.drop('text', axis=1)\n",
    "    filename_write = os.path.join(path, \"df_business1_reviews_processed.csv\")\n",
    "    business1_reviews_df.to_csv(filename_write, index=False, encoding='utf-8')\n",
    "    print(\"Wrote file to {}\".format(filename_write))\n",
    "else:\n",
    "    filename_read = os.path.join(path, \"df_business1_reviews_processed.csv\")\n",
    "    business1_reviews_df = pd.read_csv(filename_read)\n",
    "print(business1_reviews_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Process text of business2 reviews and save for model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              business_id  stars                name  \\\n",
      "0  7k9qGQyytbGxpJTnwxK6Xg      5  QDOBA Mexican Eats   \n",
      "1  7k9qGQyytbGxpJTnwxK6Xg      2  QDOBA Mexican Eats   \n",
      "2  7k9qGQyytbGxpJTnwxK6Xg      5  QDOBA Mexican Eats   \n",
      "3  7k9qGQyytbGxpJTnwxK6Xg      5  QDOBA Mexican Eats   \n",
      "4  7k9qGQyytbGxpJTnwxK6Xg      2  QDOBA Mexican Eats   \n",
      "\n",
      "                                      processed_text  \n",
      "0  like chipotle food great filling tasty got qui...  \n",
      "1  love qdoba kinda dirty especially behind count...  \n",
      "2  best burrito place world better real mexican p...  \n",
      "3  like enviornment clean food fresh taco salad g...  \n",
      "4  eat often love taco girl checkout counter toda...  \n"
     ]
    }
   ],
   "source": [
    "path = \"./business_data/\"\n",
    "filename_read = os.path.join(path, \"df_business2_reviews_processed.csv\")\n",
    "if not os.path.exists(filename_read):\n",
    "    business2_reviews_df['processed_text'] = business2_reviews_df['text'].apply(preprocess_text)\n",
    "    business2_reviews_df = business2_reviews_df.drop('text', axis=1)\n",
    "    filename_write = os.path.join(path, \"df_business2_reviews_processed.csv\")\n",
    "    business2_reviews_df.to_csv(filename_write, index=False, encoding='utf-8')\n",
    "    print(\"Wrote file to {}\".format(filename_write))\n",
    "else:\n",
    "    filename_read = os.path.join(path, \"df_business2_reviews_processed.csv\")\n",
    "    business2_reviews_df = pd.read_csv(filename_read)\n",
    "print(business2_reviews_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Process text of business3 reviews and save for model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              business_id  stars     name  \\\n",
      "0  v_vqna00z6WqKcIJZDkbAw      5  C Nails   \n",
      "1  v_vqna00z6WqKcIJZDkbAw      1  C Nails   \n",
      "2  v_vqna00z6WqKcIJZDkbAw      5  C Nails   \n",
      "3  v_vqna00z6WqKcIJZDkbAw      5  C Nails   \n",
      "4  v_vqna00z6WqKcIJZDkbAw      5  C Nails   \n",
      "\n",
      "                                      processed_text  \n",
      "0  looking new salon westchase area reading revie...  \n",
      "1  horrible service go another salon next day get...  \n",
      "2  great gel manicure current one going week stil...  \n",
      "3  gotten shellac manicure far three time though ...  \n",
      "4  reading review looking around found salon qt a...  \n"
     ]
    }
   ],
   "source": [
    "path = \"./business_data/\"\n",
    "filename_read = os.path.join(path, \"df_business3_reviews_processed.csv\")\n",
    "if not os.path.exists(filename_read):\n",
    "    business3_reviews_df['processed_text'] = business3_reviews_df['text'].apply(preprocess_text)\n",
    "    business3_reviews_df = business3_reviews_df.drop('text', axis=1)\n",
    "    filename_write = os.path.join(path, \"df_business3_reviews_processed.csv\")\n",
    "    business3_reviews_df.to_csv(filename_write, index=False, encoding='utf-8')\n",
    "    print(\"Wrote file to {}\".format(filename_write))\n",
    "else:\n",
    "    filename_read = os.path.join(path, \"df_business3_reviews_processed.csv\")\n",
    "    business3_reviews_df = pd.read_csv(filename_read)\n",
    "print(business3_reviews_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Process text of business4 reviews and save for model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              business_id  stars                       name  \\\n",
      "0  zsQ1_PNV3KN0EWhAE-WV9g      3  Tohono Chul Garden Bistro   \n",
      "1  zsQ1_PNV3KN0EWhAE-WV9g      4  Tohono Chul Garden Bistro   \n",
      "2  zsQ1_PNV3KN0EWhAE-WV9g      5  Tohono Chul Garden Bistro   \n",
      "3  zsQ1_PNV3KN0EWhAE-WV9g      3  Tohono Chul Garden Bistro   \n",
      "4  zsQ1_PNV3KN0EWhAE-WV9g      5  Tohono Chul Garden Bistro   \n",
      "\n",
      "                                      processed_text  \n",
      "0  youre looking restaurant bring town guest good...  \n",
      "1  decided try recentlyand thing looking kitchen ...  \n",
      "2  came across brunch place absolutely loved scen...  \n",
      "3  great atmosphere food flavorful older hostess ...  \n",
      "4  favorite brunch place sure prickly pear mimosa...  \n"
     ]
    }
   ],
   "source": [
    "path = \"./business_data/\"\n",
    "filename_read = os.path.join(path, \"df_business4_reviews_processed.csv\")\n",
    "if not os.path.exists(filename_read):\n",
    "    business4_reviews_df['processed_text'] = business4_reviews_df['text'].apply(preprocess_text)\n",
    "    business4_reviews_df = business4_reviews_df.drop('text', axis=1)\n",
    "    filename_write = os.path.join(path, \"df_business4_reviews_processed.csv\")\n",
    "    business4_reviews_df.to_csv(filename_write, index=False, encoding='utf-8')\n",
    "    print(\"Wrote file to {}\".format(filename_write))\n",
    "else:\n",
    "    filename_read = os.path.join(path, \"df_business4_reviews_processed.csv\")\n",
    "    business4_reviews_df = pd.read_csv(filename_read)\n",
    "print(business4_reviews_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Process text of business5 reviews and save for model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              business_id  stars            name  \\\n",
      "0  v2L2HnZzYvHPgFcVBg2TUw      1  Summerland Inn   \n",
      "1  v2L2HnZzYvHPgFcVBg2TUw      1  Summerland Inn   \n",
      "2  v2L2HnZzYvHPgFcVBg2TUw      5  Summerland Inn   \n",
      "3  v2L2HnZzYvHPgFcVBg2TUw      3  Summerland Inn   \n",
      "4  v2L2HnZzYvHPgFcVBg2TUw      5  Summerland Inn   \n",
      "\n",
      "                                      processed_text  \n",
      "0  stayed one night never come back toilet didnt ...  \n",
      "1  thing wrong room large spider web outside room...  \n",
      "2  like place got character degree outside weeken...  \n",
      "3  place interesting cute outside cottage looking...  \n",
      "4  summerland inn beautiful owner mayis sweet lad...  \n"
     ]
    }
   ],
   "source": [
    "path = \"./business_data/\"\n",
    "filename_read = os.path.join(path, \"df_business5_reviews_processed.csv\")\n",
    "if not os.path.exists(filename_read):\n",
    "    business5_reviews_df['processed_text'] = business5_reviews_df['text'].apply(preprocess_text)\n",
    "    business5_reviews_df = business5_reviews_df.drop('text', axis=1)\n",
    "    filename_write = os.path.join(path, \"df_business5_reviews_processed.csv\")\n",
    "    business5_reviews_df.to_csv(filename_write, index=False, encoding='utf-8')\n",
    "    print(\"Wrote file to {}\".format(filename_write))\n",
    "else:\n",
    "    filename_read = os.path.join(path, \"df_business5_reviews_processed.csv\")\n",
    "    business5_reviews_df = pd.read_csv(filename_read)\n",
    "print(business5_reviews_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Preprocess reviews*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   stars                                  preprocessed_text\n",
      "0      3  decide eat aware going take hour beginning end...\n",
      "1      2  second time tried turning point location first...\n",
      "2      4  place cute staff friendly nice menu good brunc...\n",
      "3      3  came saturday morning waiting month opening ho...\n",
      "4      2  mediocre best decor nice like restaurant tryin...\n"
     ]
    }
   ],
   "source": [
    "df['preprocessed_text'] = df['text'].apply(preprocess_text) # apply preprocess_text function to text column\n",
    "df = df.drop('text', axis=1)                                # drop original text column\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Save preprocessed dataframe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote file to ./data/df_preprocessed.csv\n"
     ]
    }
   ],
   "source": [
    "path = \"./data/\"\n",
    "filename_write = os.path.join(path, \"df_preprocessed.csv\")\n",
    "df.to_csv(filename_write, index=False, encoding='utf-8') # using default encoding also worked -> used for next cell but wasn't helpful\n",
    "print(\"Wrote file to {}\".format(filename_write))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Optional start point*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'path = \"./data/\"\\nfilename_read = os.path.join(path, \"df_preprocessed.csv\")\\ndf = pd.read_csv(filename_read)'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Havent figured this out yet.  I tried loading the csv file but I get an error in the next cell.  Tried different encoding but didn't work either.  Will try again later maybe.\n",
    "# Would be a nice starting point for the next step of the project because preprocessing take awhile.\n",
    "'''path = \"./data/\"\n",
    "filename_read = os.path.join(path, \"df_preprocessed.csv\")\n",
    "df = pd.read_csv(filename_read)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Vectorize reviews*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m sk_text\u001b[38;5;241m.\u001b[39mTfidfVectorizer(min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.01\u001b[39m, max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.99\u001b[39m, max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;66;03m# can adjust max_features if encounter memory issues; dtype to reduce memory usage -> defaults to float64\u001b[39;00m\n\u001b[0;32m      2\u001b[0m corpus \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m]                                         \u001b[38;5;66;03m# put preprocessed text into corpus\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m matrix \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(corpus)                                \u001b[38;5;66;03m# fit and transform the corpus\u001b[39;00m\n\u001b[0;32m      4\u001b[0m tfidf_data \u001b[38;5;241m=\u001b[39m matrix\u001b[38;5;241m.\u001b[39mtoarray()                                            \u001b[38;5;66;03m# convert matrix to array\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape:\u001b[39m\u001b[38;5;124m'\u001b[39m, tfidf_data\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2126\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2121\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2122\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2123\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2124\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2125\u001b[0m )\n\u001b[1;32m-> 2126\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_transform(raw_documents)\n\u001b[0;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2128\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2129\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1375\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1376\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1377\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1378\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1379\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1380\u001b[0m             )\n\u001b[0;32m   1381\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1383\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1386\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\andre\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1281\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m   1278\u001b[0m         \u001b[38;5;66;03m# Ignore out-of-vocabulary items for fixed_vocab=True\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 1281\u001b[0m j_indices\u001b[38;5;241m.\u001b[39mextend(feature_counter\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m   1282\u001b[0m values\u001b[38;5;241m.\u001b[39mextend(feature_counter\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m   1283\u001b[0m indptr\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(j_indices))\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vectorizer = sk_text.TfidfVectorizer(min_df=.01, max_df=.99, max_features=350, dtype=np.float32) # can adjust max_features if encounter memory issues; dtype to reduce memory usage -> defaults to float64\n",
    "corpus = df['preprocessed_text']                                         # put preprocessed text into corpus\n",
    "matrix = vectorizer.fit_transform(corpus)                                # fit and transform the corpus\n",
    "tfidf_data = matrix.toarray()                                            # convert matrix to array\n",
    "print('shape:', tfidf_data.shape)\n",
    "print(tfidf_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Feature names*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able' 'absolutely' 'actually' 'almost' 'also' 'always' 'amazing'\n",
      " 'another' 'anything' 'appetizer' 'area' 'around' 'arrived' 'ask' 'asked'\n",
      " 'atmosphere' 'attentive' 'away' 'awesome' 'back' 'bad' 'bar' 'bartender'\n",
      " 'beautiful' 'beef' 'beer' 'best' 'better' 'big' 'bit' 'bread' 'breakfast'\n",
      " 'bring' 'brought' 'burger' 'business' 'busy' 'cake' 'call' 'called'\n",
      " 'came' 'cant' 'car' 'care' 'check' 'cheese' 'chicken' 'chip' 'choice'\n",
      " 'city' 'clean' 'close' 'coffee' 'cold' 'come' 'coming' 'cooked' 'cool'\n",
      " 'could' 'couldnt' 'couple' 'crab' 'cream' 'customer' 'day' 'decent'\n",
      " 'decided' 'definitely' 'delicious' 'dessert' 'didnt' 'different' 'dining'\n",
      " 'dinner' 'disappointed' 'dish' 'dog' 'done' 'dont' 'door' 'drink' 'eat'\n",
      " 'eating' 'egg' 'else' 'employee' 'end' 'enjoy' 'enjoyed' 'enough'\n",
      " 'entree' 'especially' 'even' 'ever' 'every' 'everyone' 'everything'\n",
      " 'excellent' 'experience' 'extra' 'extremely' 'family' 'fantastic' 'far'\n",
      " 'fast' 'favorite' 'feel' 'felt' 'finally' 'find' 'first' 'fish' 'flavor'\n",
      " 'food' 'found' 'free' 'fresh' 'fried' 'friend' 'friendly' 'front' 'fry'\n",
      " 'full' 'fun' 'gave' 'get' 'getting' 'give' 'go' 'going' 'good' 'got'\n",
      " 'great' 'group' 'guy' 'half' 'hand' 'happy' 'hard' 'help' 'helpful'\n",
      " 'high' 'highly' 'home' 'hot' 'hotel' 'hour' 'house' 'however' 'huge'\n",
      " 'husband' 'ice' 'id' 'ill' 'im' 'inside' 'issue' 'item' 'ive' 'job'\n",
      " 'keep' 'kid' 'kind' 'know' 'large' 'last' 'later' 'le' 'least' 'left'\n",
      " 'let' 'like' 'line' 'little' 'live' 'local' 'location' 'long' 'look'\n",
      " 'looked' 'looking' 'lot' 'love' 'loved' 'lunch' 'made' 'make' 'manager'\n",
      " 'many' 'may' 'maybe' 'meal' 'meat' 'menu' 'minute' 'money' 'month' 'much'\n",
      " 'music' 'must' 'name' 'need' 'needed' 'never' 'new' 'next' 'nice' 'night'\n",
      " 'nothing' 'offer' 'ok' 'old' 'one' 'open' 'option' 'order' 'ordered'\n",
      " 'outside' 'overall' 'owner' 'parking' 'part' 'party' 'pay' 'people'\n",
      " 'perfect' 'person' 'pizza' 'place' 'plate' 'pm' 'point' 'pork' 'portion'\n",
      " 'potato' 'pretty' 'price' 'probably' 'problem' 'put' 'quality' 'quick'\n",
      " 'quite' 'really' 'recommend' 'reservation' 'restaurant' 'return' 'review'\n",
      " 'rice' 'right' 'roll' 'room' 'said' 'salad' 'sandwich' 'sat' 'sauce'\n",
      " 'say' 'seated' 'seating' 'second' 'see' 'seemed' 'selection' 'served'\n",
      " 'server' 'service' 'several' 'shop' 'shrimp' 'side' 'since' 'size'\n",
      " 'small' 'someone' 'something' 'soup' 'special' 'spicy' 'spot' 'staff'\n",
      " 'star' 'started' 'stay' 'steak' 'still' 'stop' 'store' 'street' 'super'\n",
      " 'sure' 'sushi' 'sweet' 'table' 'taco' 'take' 'taste' 'tasted' 'tasty'\n",
      " 'tell' 'thats' 'there' 'thing' 'think' 'though' 'thought' 'three' 'time'\n",
      " 'today' 'told' 'took' 'top' 'town' 'tried' 'try' 'trying' 'two' 'use'\n",
      " 'used' 'usually' 'visit' 'wait' 'waited' 'waiter' 'waiting' 'waitress'\n",
      " 'walk' 'walked' 'want' 'wanted' 'wasnt' 'water' 'way' 'week' 'well'\n",
      " 'went' 'whole' 'wife' 'wine' 'wing' 'without' 'wonderful' 'wont' 'work'\n",
      " 'worth' 'would' 'wrong' 'year' 'youre']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Concatenate stars and matrix into new dataframe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   stars    0    1    2    3    4    5    6         7         8  ...  340  \\\n",
      "0      3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.168147  0.000000  ...  0.0   \n",
      "1      2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000  ...  0.0   \n",
      "2      4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.262300  0.000000  ...  0.0   \n",
      "3      3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.128253  0.000000  ...  0.0   \n",
      "4      2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.129936  ...  0.0   \n",
      "\n",
      "   341  342  343  344       345       346       347  348  349  \n",
      "0  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0  \n",
      "1  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0  \n",
      "2  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0  \n",
      "3  0.0  0.0  0.0  0.0  0.000000  0.090665  0.000000  0.0  0.0  \n",
      "4  0.0  0.0  0.0  0.0  0.118585  0.000000  0.141233  0.0  0.0  \n",
      "\n",
      "[5 rows x 351 columns]\n"
     ]
    }
   ],
   "source": [
    "assert len(df) == tfidf_data.shape[0], \"Number of rows in dataframe does not match number of rows in matrix.\" # check number of rows in dataframe equals number of rows in tfidf matrix\n",
    "df_data = pd.concat([df[['stars']], pd.DataFrame(tfidf_data)], axis=1)                                        # concatenate stars column with tfidf matrix\n",
    "print(df_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Add featured names into dataframe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   stars  able  absolutely  actually  almost  also  always  amazing   another  \\\n",
      "0      3   0.0         0.0       0.0     0.0   0.0     0.0      0.0  0.168147   \n",
      "1      2   0.0         0.0       0.0     0.0   0.0     0.0      0.0  0.000000   \n",
      "2      4   0.0         0.0       0.0     0.0   0.0     0.0      0.0  0.262300   \n",
      "3      3   0.0         0.0       0.0     0.0   0.0     0.0      0.0  0.128253   \n",
      "4      2   0.0         0.0       0.0     0.0   0.0     0.0      0.0  0.000000   \n",
      "\n",
      "   anything  ...  wing  without  wonderful  wont  work     worth     would  \\\n",
      "0  0.000000  ...   0.0      0.0        0.0   0.0   0.0  0.000000  0.000000   \n",
      "1  0.000000  ...   0.0      0.0        0.0   0.0   0.0  0.000000  0.000000   \n",
      "2  0.000000  ...   0.0      0.0        0.0   0.0   0.0  0.000000  0.000000   \n",
      "3  0.000000  ...   0.0      0.0        0.0   0.0   0.0  0.000000  0.090665   \n",
      "4  0.129936  ...   0.0      0.0        0.0   0.0   0.0  0.118585  0.000000   \n",
      "\n",
      "      wrong  year  youre  \n",
      "0  0.000000   0.0    0.0  \n",
      "1  0.000000   0.0    0.0  \n",
      "2  0.000000   0.0    0.0  \n",
      "3  0.000000   0.0    0.0  \n",
      "4  0.141233   0.0    0.0  \n",
      "\n",
      "[5 rows x 351 columns]\n"
     ]
    }
   ],
   "source": [
    "df_data.columns = ['stars'] + feature_names.tolist()\n",
    "print(df_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Save dataframe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote file to ./data/df_data.csv\n"
     ]
    }
   ],
   "source": [
    "path = \"./data/\"\n",
    "filename_write = os.path.join(path, \"df_data.csv\")\n",
    "df_data.to_csv(filename_write, index=False)\n",
    "print(\"Wrote file to {}\".format(filename_write))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
