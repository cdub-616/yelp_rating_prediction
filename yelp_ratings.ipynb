{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Yelp Rating Prediction Using Tensorflow</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn.feature_extraction.text as sk_text\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Get reviews*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                review_id                 user_id             business_id  \\\n",
      "0  KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA  XQfwVwDr-v0ZS3_CbbE5Xw   \n",
      "1  BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q  7ATYjTIgM3jUlt4UM3IypQ   \n",
      "2  saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A  YjUWPpI6HXG530lwP-fb2A   \n",
      "3  AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA   \n",
      "4  Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ  e4Vwtrqf-wpJfwesgvdgxQ   \n",
      "\n",
      "   stars  useful  funny  cool  \\\n",
      "0      3       0      0     0   \n",
      "1      5       1      0     1   \n",
      "2      3       0      0     0   \n",
      "3      5       1      0     1   \n",
      "4      4       1      0     1   \n",
      "\n",
      "                                                text                date  \n",
      "0  If you decide to eat here, just be aware it is... 2018-07-07 22:09:11  \n",
      "1  I've taken a lot of spin classes over the year... 2012-01-03 15:28:18  \n",
      "2  Family diner. Had the buffet. Eclectic assortm... 2014-02-05 20:30:30  \n",
      "3  Wow!  Yummy, different,  delicious.   Our favo... 2015-01-04 00:01:03  \n",
      "4  Cute interior and owner (?) gave us tour of up... 2017-01-14 20:54:15  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json('data/yelp_academic_dataset_review.json', lines=True, chunksize=1000) # smaller chunksize helps with memory issues\n",
    "df = pd.concat(df)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Clean reviews*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   stars                                               text\n",
      "0      3  If you decide to eat here, just be aware it is...\n",
      "1      2  This is the second time we tried turning point...\n",
      "2      4  The place is cute and the staff was very frien...\n",
      "3      3  We came on a Saturday morning after waiting a ...\n",
      "4      2  Mediocre at best. The decor is very nice, and ...\n"
     ]
    }
   ],
   "source": [
    "# Convert all missing values and zeroes in specified column to median -> from labs with slight modifications\n",
    "def missing_median(df, name):\n",
    "    non_zero_values = df[name][df[name] != 0]\n",
    "    med = non_zero_values.median()\n",
    "    df[name] = df[name].replace(0, med)\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "business_review_count = df.groupby('business_id').size().reset_index(name='review_count')                        # count reviews for each business\n",
    "businesses_with_20 = business_review_count[business_review_count['review_count'] >= 20]                          # filter businesses with 20 or more reviews\n",
    "df = pd.merge(df, businesses_with_20, on='business_id', how='inner')                                             # merge with reviews to get only reviews for businesses with 20 or more reviews\n",
    "df = df.drop(['business_id', 'review_id', 'user_id', 'funny', 'cool', 'useful', 'date', 'review_count'], axis=1) # drop unnecessary columns\n",
    "missing_median(df, 'stars')                                                                                      # in case there are any reviews with 0 or missing stars, replace with median\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Preprocess reviews*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   stars                                  preprocessed_text\n",
      "0      3  decide eat aware going take hour beginning end...\n",
      "1      2  second time tried turning point location first...\n",
      "2      4  place cute staff friendly nice menu good brunc...\n",
      "3      3  came saturday morning waiting month opening ho...\n",
      "4      2  mediocre best decor nice like restaurant tryin...\n"
     ]
    }
   ],
   "source": [
    "# preprocess text function -> if you get nltk error:  open Anaconda prompt -> ipython -> import nltk -> nltk.download('popular') OR command line -> python -m nltk.downloader popular\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()                 # lowercase\n",
    "    text = re.sub(r'\\d+', '', text)     # remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text)    # remove extra whitespace\n",
    "    text = text.strip()                 # remove leading/trailing whitespace\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]          # remove stopwords\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens] # lemmatize\n",
    "    filtered_text = ' '.join(lemmatized_tokens)                                    # join back into text\n",
    "    return filtered_text\n",
    "\n",
    "df['preprocessed_text'] = df['text'].apply(preprocess_text) # apply preprocess_text function to text column\n",
    "df = df.drop('text', axis=1)                                # drop original text column\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Save preprocessed dataframe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote file to ./data/df_preprocessed.csv\n"
     ]
    }
   ],
   "source": [
    "path = \"./data/\"\n",
    "filename_write = os.path.join(path, \"df_preprocessed.csv\")\n",
    "df.to_csv(filename_write, index=False, encoding='utf-8') # using default encoding also worked -> used for next cell but wasn't helpful\n",
    "print(\"Wrote file to {}\".format(filename_write))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Optional start point*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Havent figured this out yet.  I tried loading the csv file but I get an error in the next cell.  Tried different encoding but didn't work either.  Will try again later maybe.\n",
    "# Would be a nice starting point for the next step of the project because preprocessing take awhile.\n",
    "'''path = \"./data/\"\n",
    "filename_read = os.path.join(path, \"df_preprocessed.csv\")\n",
    "df = pd.read_csv(filename_read)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Vectorize Reviews*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (6146631, 400)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = sk_text.TfidfVectorizer(max_features=400, dtype=np.float32) # can adjust max_features if encounter memory issues; dtype to reduce memory usage -> defaults to float64\n",
    "corpus = df['preprocessed_text']                                         # put preprocessed text into corpus\n",
    "matrix = vectorizer.fit_transform(corpus)                                # fit and transform the corpus\n",
    "tfidf_data = matrix.toarray()                                            # convert matrix to array\n",
    "print('shape:', tfidf_data.shape)\n",
    "print(tfidf_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Feature names*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able' 'absolutely' 'actually' 'almost' 'also' 'always' 'amazing'\n",
      " 'amount' 'another' 'anyone' 'anything' 'appetizer' 'area' 'around'\n",
      " 'arrived' 'ask' 'asked' 'atmosphere' 'attentive' 'away' 'awesome' 'back'\n",
      " 'bacon' 'bad' 'bar' 'bartender' 'bbq' 'bean' 'beautiful' 'beef' 'beer'\n",
      " 'best' 'better' 'big' 'bit' 'bite' 'bowl' 'bread' 'breakfast' 'bring'\n",
      " 'brought' 'brunch' 'burger' 'business' 'busy' 'cake' 'call' 'called'\n",
      " 'came' 'cant' 'car' 'care' 'check' 'cheese' 'chicken' 'chip' 'chocolate'\n",
      " 'choice' 'city' 'clean' 'close' 'cocktail' 'coffee' 'cold' 'come'\n",
      " 'coming' 'cooked' 'cool' 'could' 'couldnt' 'couple' 'course' 'crab'\n",
      " 'cream' 'customer' 'cut' 'day' 'deal' 'decent' 'decided' 'definitely'\n",
      " 'delicious' 'dessert' 'didnt' 'different' 'dining' 'dinner'\n",
      " 'disappointed' 'dish' 'doesnt' 'dog' 'done' 'dont' 'door' 'drink' 'eat'\n",
      " 'eating' 'egg' 'either' 'else' 'employee' 'end' 'enjoy' 'enjoyed'\n",
      " 'enough' 'entree' 'especially' 'even' 'ever' 'every' 'everyone'\n",
      " 'everything' 'excellent' 'experience' 'extra' 'extremely' 'family' 'fan'\n",
      " 'fantastic' 'far' 'fast' 'favorite' 'feel' 'felt' 'finally' 'find' 'fine'\n",
      " 'first' 'fish' 'flavor' 'food' 'found' 'free' 'french' 'fresh' 'fried'\n",
      " 'friend' 'friendly' 'front' 'fry' 'full' 'fun' 'gave' 'get' 'getting'\n",
      " 'give' 'go' 'going' 'good' 'got' 'great' 'green' 'group' 'guy' 'hair'\n",
      " 'half' 'hand' 'happy' 'hard' 'help' 'helpful' 'high' 'highly' 'home'\n",
      " 'hot' 'hotel' 'hour' 'house' 'however' 'huge' 'husband' 'ice' 'id' 'ill'\n",
      " 'im' 'inside' 'instead' 'isnt' 'issue' 'item' 'ive' 'job' 'keep' 'kid'\n",
      " 'kind' 'know' 'large' 'last' 'later' 'le' 'least' 'leave' 'left' 'let'\n",
      " 'light' 'like' 'line' 'little' 'live' 'local' 'location' 'long' 'look'\n",
      " 'looked' 'looking' 'lot' 'love' 'loved' 'lunch' 'made' 'make' 'making'\n",
      " 'manager' 'many' 'may' 'maybe' 'meal' 'meat' 'menu' 'minute' 'money'\n",
      " 'month' 'much' 'music' 'must' 'nail' 'name' 'need' 'needed' 'never' 'new'\n",
      " 'next' 'nice' 'night' 'nothing' 'offer' 'oh' 'ok' 'old' 'one' 'onion'\n",
      " 'open' 'option' 'order' 'ordered' 'outside' 'overall' 'owner' 'parking'\n",
      " 'part' 'party' 'pay' 'people' 'perfect' 'perfectly' 'person' 'philly'\n",
      " 'phone' 'pick' 'piece' 'pizza' 'place' 'plate' 'pm' 'point' 'pork'\n",
      " 'portion' 'potato' 'pretty' 'price' 'probably' 'problem' 'put' 'quality'\n",
      " 'quick' 'quickly' 'quite' 'really' 'reason' 'recommend' 'red' 'regular'\n",
      " 'reservation' 'restaurant' 'return' 'review' 'rice' 'right' 'roll' 'room'\n",
      " 'rude' 'said' 'salad' 'sandwich' 'sat' 'sauce' 'saw' 'say' 'seated'\n",
      " 'seating' 'second' 'see' 'seemed' 'selection' 'served' 'server' 'service'\n",
      " 'several' 'shop' 'shrimp' 'side' 'since' 'sit' 'size' 'small' 'someone'\n",
      " 'something' 'soup' 'special' 'spicy' 'spot' 'st' 'staff' 'star' 'start'\n",
      " 'started' 'stay' 'steak' 'still' 'stop' 'stopped' 'store' 'street'\n",
      " 'super' 'sure' 'sushi' 'sweet' 'table' 'taco' 'take' 'taste' 'tasted'\n",
      " 'tasty' 'tea' 'tell' 'thank' 'thats' 'there' 'thing' 'think' 'though'\n",
      " 'thought' 'three' 'time' 'today' 'told' 'took' 'top' 'town' 'tried'\n",
      " 'trip' 'try' 'trying' 'two' 'use' 'used' 'usually' 'visit' 'wait'\n",
      " 'waited' 'waiter' 'waiting' 'waitress' 'walk' 'walked' 'want' 'wanted'\n",
      " 'wasnt' 'water' 'way' 'week' 'well' 'went' 'whole' 'wife' 'wine' 'wing'\n",
      " 'wish' 'without' 'wonderful' 'wont' 'work' 'working' 'worth' 'would'\n",
      " 'wouldnt' 'wrong' 'year' 'yet' 'youre']\n",
      "Wrote file to ./data/featured_names.csv\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Concatenate stars and matrix into new dataframe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   stars    0    1    2    3    4    5    6    7         8  ...  390  391  \\\n",
      "0      3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.168147  ...  0.0  0.0   \n",
      "1      2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...  0.0  0.0   \n",
      "2      4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.207039  ...  0.0  0.0   \n",
      "3      3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.128253  ...  0.0  0.0   \n",
      "4      2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...  0.0  0.0   \n",
      "\n",
      "   392       393       394  395       396  397  398  399  \n",
      "0  0.0  0.000000  0.000000  0.0  0.000000  0.0  0.0  0.0  \n",
      "1  0.0  0.000000  0.000000  0.0  0.000000  0.0  0.0  0.0  \n",
      "2  0.0  0.000000  0.000000  0.0  0.000000  0.0  0.0  0.0  \n",
      "3  0.0  0.000000  0.090665  0.0  0.000000  0.0  0.0  0.0  \n",
      "4  0.0  0.117374  0.000000  0.0  0.139791  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 401 columns]\n"
     ]
    }
   ],
   "source": [
    "assert len(df) == tfidf_data.shape[0], \"Number of rows in dataframe does not match number of rows in matrix.\" # check number of rows in dataframe equals number of rows in tfidf matrix\n",
    "df_data = pd.concat([df[['stars']], pd.DataFrame(tfidf_data)], axis=1)                                        # concatenate stars column with tfidf matrix\n",
    "print(df_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Add featured names into dataframe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   stars  able  absolutely  actually  almost  also  always  amazing  amount  \\\n",
      "0      3   0.0         0.0       0.0     0.0   0.0     0.0      0.0     0.0   \n",
      "1      2   0.0         0.0       0.0     0.0   0.0     0.0      0.0     0.0   \n",
      "2      4   0.0         0.0       0.0     0.0   0.0     0.0      0.0     0.0   \n",
      "3      3   0.0         0.0       0.0     0.0   0.0     0.0      0.0     0.0   \n",
      "4      2   0.0         0.0       0.0     0.0   0.0     0.0      0.0     0.0   \n",
      "\n",
      "    another  ...  wont  work  working     worth     would  wouldnt     wrong  \\\n",
      "0  0.168147  ...   0.0   0.0      0.0  0.000000  0.000000      0.0  0.000000   \n",
      "1  0.000000  ...   0.0   0.0      0.0  0.000000  0.000000      0.0  0.000000   \n",
      "2  0.207039  ...   0.0   0.0      0.0  0.000000  0.000000      0.0  0.000000   \n",
      "3  0.128253  ...   0.0   0.0      0.0  0.000000  0.090665      0.0  0.000000   \n",
      "4  0.000000  ...   0.0   0.0      0.0  0.117374  0.000000      0.0  0.139791   \n",
      "\n",
      "   year  yet  youre  \n",
      "0   0.0  0.0    0.0  \n",
      "1   0.0  0.0    0.0  \n",
      "2   0.0  0.0    0.0  \n",
      "3   0.0  0.0    0.0  \n",
      "4   0.0  0.0    0.0  \n",
      "\n",
      "[5 rows x 401 columns]\n"
     ]
    }
   ],
   "source": [
    "df_data.columns = ['stars'] + feature_names.tolist()\n",
    "print(df_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Save dataframe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote file to ./data/df_data.csv\n"
     ]
    }
   ],
   "source": [
    "filename_write = os.path.join(path, \"df_data.csv\")\n",
    "df_data.to_csv(filename_write, index=False)\n",
    "print(\"Wrote file to {}\".format(filename_write))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
